{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Frequentist Neural Network for Classification\n",
        "\n",
        "# Required Libraries\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "U_lJTqdFhWMM",
        "outputId": "b3a8187d-7014-4470-8de8-2ffad4a48892"
      },
      "outputs": [],
      "source": [
        "class SimpleFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout after first layer\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)  # Dropout applied here\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "BC_c8paShX--"
      },
      "outputs": [],
      "source": [
        "# # uncomment for Cancer Dataset\n",
        "\n",
        "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
        "#     df = pd.read_csv(file_path)\n",
        "#     print(\"Loaded dataset with shape:\", df.shape)\n",
        "\n",
        "#     # Define target and feature columns correctly\n",
        "#     target_col = \"diagnosis\"\n",
        "#     feature_cols = df.columns[2:]  # Exclude 'id' (first column) and take features from column 3 onward\n",
        "\n",
        "#     # Extract features (X) and target labels (y)\n",
        "#     X = df[feature_cols].values\n",
        "#     y = df[target_col].values\n",
        "\n",
        "#     # Encode 'diagnosis' column ('M' -> 1, 'B' -> 0)\n",
        "#     le = LabelEncoder()\n",
        "#     y = le.fit_transform(y)  # Converts \"M\"/\"B\" to 1/0\n",
        "\n",
        "#     # Standardize features\n",
        "#     scaler = StandardScaler()\n",
        "#     X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#     # Check for NaNs and replace if necessary\n",
        "#     if np.isnan(X_scaled).any():\n",
        "#         print(\"âš ï¸ Warning: NaNs found in dataset! Replacing with zeros.\")\n",
        "#         X_scaled = np.nan_to_num(X_scaled)\n",
        "\n",
        "#     # Print class distribution\n",
        "#     unique, counts = np.unique(y, return_counts=True)\n",
        "#     print(\"Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "#     # Ensure valid test size\n",
        "#     min_class_samples = min(counts)\n",
        "#     test_size = min(0.3, max(0.1, (min_class_samples - 1) / len(y)))\n",
        "\n",
        "#     # Perform train-test split\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X_scaled, y, test_size=test_size, random_state=random_state, stratify=y if min_class_samples > 1 else None\n",
        "#     )\n",
        "\n",
        "#     # Convert to PyTorch tensors\n",
        "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "#     y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "#     X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "#     y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "#     input_dim = X_train_tensor.shape[1]\n",
        "#     output_dim = len(np.unique(y))  # Should be 2 (Benign/Malignant)\n",
        "\n",
        "#     return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, input_dim, output_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # uncomment for Diabetes datset\n",
        "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
        "#     # Load dataset\n",
        "#     df = pd.read_csv(file_path)\n",
        "#     print(\"Loaded dataset with shape:\", df.shape)\n",
        "\n",
        "#     # Define target and feature columns\n",
        "#     target_col = \"Outcome\"\n",
        "#     feature_cols = df.columns[:-1]  # Use all columns except 'Outcome' as features\n",
        "\n",
        "#     # Extract features (X) and target labels (y)\n",
        "#     X = df[feature_cols].values\n",
        "#     y = df[target_col].values  # Already numeric (0 or 1), no encoding needed\n",
        "\n",
        "#     # Check for missing values and replace NaNs with column means\n",
        "#     df.replace(0, np.nan, inplace=True)  # Convert zeros to NaN (common in medical datasets)\n",
        "#     df.fillna(df.mean(), inplace=True)   # Replace NaNs with mean values\n",
        "\n",
        "#     # Standardize features\n",
        "#     scaler = StandardScaler()\n",
        "#     X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#     # Print class distribution\n",
        "#     unique, counts = np.unique(y, return_counts=True)\n",
        "#     print(\"Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "#     # Ensure valid test size\n",
        "#     min_class_samples = min(counts)\n",
        "#     test_size = min(0.3, max(0.1, (min_class_samples - 1) / len(y)))\n",
        "\n",
        "#     # Perform train-test split\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X_scaled, y, test_size=test_size, random_state=random_state, stratify=y if min_class_samples > 1 else None\n",
        "#     )\n",
        "\n",
        "#     # Convert to PyTorch tensors\n",
        "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "#     y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "#     X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "#     y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "#     input_dim = X_train_tensor.shape[1]\n",
        "#     output_dim = len(np.unique(y))  # Should be 2 (Diabetic/Non-Diabetic)\n",
        "\n",
        "#     return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, input_dim, output_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment for heart_statlog_cleveland_hungary_final Dataset\n",
        "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
        "#     # Load dataset\n",
        "#     df = pd.read_csv(file_path)\n",
        "#     print(\"Loaded dataset with shape:\", df.shape)\n",
        "\n",
        "#     # Define target and feature columns\n",
        "#     target_col = \"target\"\n",
        "#     feature_cols = df.columns[:-1]  # Use all columns except 'target' as features\n",
        "\n",
        "#     # Extract features (X) and target labels (y)\n",
        "#     X = df[feature_cols]\n",
        "#     y = df[target_col].values  \n",
        "\n",
        "#     # Identify categorical columns (if any)\n",
        "#     categorical_cols = [\"sex\", \"chest pain type\", \"fasting blood sugar\", \"resting ecg\", \"exercise angina\", \"ST slope\"]\n",
        "    \n",
        "#     # Convert categorical columns to numerical using one-hot encoding\n",
        "#     X = pd.get_dummies(X, columns=categorical_cols)\n",
        "\n",
        "#     # Handle missing values\n",
        "#     X.replace(0, np.nan, inplace=True)  # Convert zeros to NaN (common in medical datasets)\n",
        "#     X.fillna(X.mean(), inplace=True)    # Replace NaNs with column means\n",
        "\n",
        "#     # Standardize numerical features\n",
        "#     scaler = StandardScaler()\n",
        "#     X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#     # Check class distribution\n",
        "#     unique, counts = np.unique(y, return_counts=True)\n",
        "#     print(\"Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "#     # Ensure valid test size\n",
        "#     min_class_samples = min(counts)\n",
        "#     test_size = min(0.3, max(0.1, (min_class_samples - 1) / len(y)))\n",
        "\n",
        "#     # Perform train-test split\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X_scaled, y, test_size=test_size, random_state=random_state, stratify=y if min_class_samples > 1 else None\n",
        "#     )\n",
        "\n",
        "#     # Convert to PyTorch tensors\n",
        "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "#     y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "#     X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "#     y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "#     input_dim = X_train_tensor.shape[1]\n",
        "#     output_dim = len(np.unique(y))  # Should be 2 if binary classification, else check for multi-class\n",
        "\n",
        "#     return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, input_dim, output_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment for heart Dataset\n",
        "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
        "#     # Load dataset\n",
        "#     df = pd.read_csv(file_path)\n",
        "#     print(\"Loaded dataset with shape:\", df.shape)\n",
        "\n",
        "#     # Define target and feature columns\n",
        "#     target_col = \"target\"\n",
        "#     feature_cols = df.columns[:-1]  # Use all columns except 'target' as features\n",
        "\n",
        "#     # Extract features (X) and target labels (y)\n",
        "#     X = df[feature_cols]\n",
        "#     y = df[target_col].values  \n",
        "\n",
        "#     # Identify categorical columns (if any)\n",
        "#     categorical_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"]\n",
        "    \n",
        "#     # Convert categorical columns to numerical using one-hot encoding\n",
        "#     X = pd.get_dummies(X, columns=categorical_cols)\n",
        "\n",
        "#     # Handle missing values\n",
        "#     X.replace(0, np.nan, inplace=True)  # Convert zeros to NaN (common in medical datasets)\n",
        "#     X.fillna(X.mean(), inplace=True)    # Replace NaNs with column means\n",
        "\n",
        "#     # Standardize numerical features\n",
        "#     scaler = StandardScaler()\n",
        "#     X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#     # Check class distribution\n",
        "#     unique, counts = np.unique(y, return_counts=True)\n",
        "#     print(\"Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "#     # Ensure valid test size\n",
        "#     min_class_samples = min(counts)\n",
        "#     test_size = min(0.3, max(0.1, (min_class_samples - 1) / len(y)))\n",
        "\n",
        "#     # Perform train-test split\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X_scaled, y, test_size=test_size, random_state=random_state, stratify=y if min_class_samples > 1 else None\n",
        "#     )\n",
        "\n",
        "#     # Convert to PyTorch tensors\n",
        "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "#     y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "#     X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "#     y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "#     input_dim = X_train_tensor.shape[1]\n",
        "#     output_dim = len(np.unique(y))  # Should be 2 if binary classification, else check for multi-class\n",
        "\n",
        "#     return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, input_dim, output_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # uncomment for hepatitis Dataset\n",
        "# def load_and_preprocess_data(file_path, test_size=0.3, random_state=42):\n",
        "#     # Load dataset, replacing \"?\" with NaN\n",
        "#     df = pd.read_csv(file_path, na_values=\"?\")\n",
        "#     print(\"Loaded dataset with shape:\", df.shape)\n",
        "\n",
        "#     # Define target and feature columns\n",
        "#     target_col = \"out_class\"\n",
        "#     feature_cols = df.columns[1:]  # Use all columns except 'out_class' as features\n",
        "\n",
        "#     # Extract features (X) and target labels (y)\n",
        "#     X = df[feature_cols]\n",
        "#     y = df[target_col].values  \n",
        "\n",
        "#     # ðŸ” Convert Target Labels (out_class)\n",
        "#     print(\"\\nConverting `out_class`: 2 â†’ 1 (Survived), 1 â†’ 0 (Died)\")\n",
        "#     df[\"out_class\"] = df[\"out_class\"].replace({2: 1, 1: 0})\n",
        "#     y = df[\"out_class\"].values\n",
        "\n",
        "#     # ðŸ” Convert Binary Features (1 â†’ 0 \"No\", 2 â†’ 1 \"Yes\")\n",
        "#     binary_cols = [\"sex\", \"steroid\", \"antivirals\", \"fatigue\", \"malaise\", \"anorexia\",\n",
        "#                    \"liver_big\", \"liver_firm\", \"spleen_palable\", \"spiders\", \"ascites\",\n",
        "#                    \"varices\", \"histology\"]\n",
        "\n",
        "#     print(\"\\nConverting binary categorical columns: 1 â†’ 0 (No), 2 â†’ 1 (Yes)\")\n",
        "#     df[binary_cols] = df[binary_cols].replace({1: 0, 2: 1})\n",
        "\n",
        "#     # ðŸ” Handle Missing Values\n",
        "#     print(\"\\nReplacing missing values with column means...\")\n",
        "#     df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "#     # Standardize numerical features\n",
        "#     numerical_cols = [\"age\", \"bilirubin\", \"alk_phosphate\", \"sgot\", \"albumin\", \"protime\"]\n",
        "#     scaler = StandardScaler()\n",
        "#     df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "#     # Extract processed feature matrix\n",
        "#     X = df[feature_cols].values\n",
        "\n",
        "#     # ðŸ” Check Class Distribution\n",
        "#     unique, counts = np.unique(y, return_counts=True)\n",
        "#     print(\"\\nFinal Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "#     # Ensure valid test size\n",
        "#     min_class_samples = min(counts)\n",
        "#     test_size = min(0.3, max(0.1, (min_class_samples - 1) / len(y)))\n",
        "\n",
        "#     # Perform train-test split\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X, y, test_size=test_size, random_state=random_state, stratify=y if min_class_samples > 1 else None\n",
        "#     )\n",
        "\n",
        "#     # Convert to PyTorch tensors\n",
        "#     X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "#     y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "#     X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "#     y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "#     input_dim = X_train_tensor.shape[1]\n",
        "#     output_dim = len(np.unique(y))  # Should be 2 for binary classification\n",
        "\n",
        "#     return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, input_dim, output_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "CEnR7GyUgOMG"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_train, y_train, epochs=100, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    outputs = model(X_train)  # No softmax!\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPPSosFkgTI0",
        "outputId": "47d8f983-62db-442b-e3f3-49c8f38ab9fe"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "        accuracy = (predicted == y_test).float().mean().item()\n",
        "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP9wGhixgSHS",
        "outputId": "3f193edf-8446-48e2-963c-0ff077522c38"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Define the datasets folder path relative to this script\n",
        "    base_path = os.path.join(os.getcwd(), \"DataSets\")\n",
        "    \n",
        "    # Specify the CSV file to use. Change this to run a different dataset.\n",
        "    # dataset_file = \"cancer.csv\"\n",
        "    dataset_file = \"diabetes.csv\"\n",
        "    # dataset_file = \"heart_statlog_cleveland_hungary_final.csv\"\n",
        "    # dataset_file = \"heart.csv\"\n",
        "    # dataset_file = \"hepatitis.csv\"\n",
        "\n",
        "\n",
        "    file_path = os.path.join(base_path, dataset_file)\n",
        "    \n",
        "    print(\"Using dataset:\", file_path)\n",
        "    \n",
        "    # Load dataset\n",
        "    # X_train, X_test, y_train, y_test, input_dim, output_dim = load_and_preprocess_data(file_path)\n",
        "\n",
        "    print(f\"Input dimension: {input_dim}, Output dimension: {output_dim}\")\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = SimpleFNN(input_dim, hidden_dim=3, output_dim=output_dim)\n",
        "\n",
        "    # Print class distribution\n",
        "    unique, counts = np.unique(y_train.numpy(), return_counts=True)\n",
        "    print(\"Train Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "    unique, counts = np.unique(y_test.numpy(), return_counts=True)\n",
        "    print(\"Test Class Distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "    model.train()  # set model to training mode\n",
        "    start_train = time.time()\n",
        "    print(\"Training model...\")\n",
        "    train_model(model, X_train, y_train, epochs=1000, lr=0.001)\n",
        "    end_train = time.time()\n",
        "    train_time = end_train - start_train\n",
        "    print(\"Training time for Freqtist NN: {:.4f} seconds\".format(train_time))\n",
        "\n",
        "    \n",
        "    start_infer = time.time()\n",
        "    print(\"Evaluating model...\")\n",
        "    evaluate_model(model, X_test, y_test)\n",
        "    end_infer = time.time()\n",
        "    infer_time = end_infer - start_infer\n",
        "    print(\"Inference time for Freqtist NN: {:.4f} seconds\".format(infer_time))\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dataset: /workspaces/paviabera/Desktop/Archive BVNN/code/DataSets/diabetes.csv\n",
            "Loaded dataset with shape: (768, 9)\n",
            "Class Distribution: {np.int64(0): np.int64(500), np.int64(1): np.int64(268)}\n",
            "Input dimension: 8, Output dimension: 2\n",
            "Train Class Distribution: {np.int64(0): np.int64(350), np.int64(1): np.int64(187)}\n",
            "Test Class Distribution: {np.int64(0): np.int64(150), np.int64(1): np.int64(81)}\n",
            "Training model...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/1000], Loss: 0.7139\n",
            "Epoch [20/1000], Loss: 0.7026\n",
            "Epoch [30/1000], Loss: 0.6939\n",
            "Epoch [40/1000], Loss: 0.6894\n",
            "Epoch [50/1000], Loss: 0.6798\n",
            "Epoch [60/1000], Loss: 0.6749\n",
            "Epoch [70/1000], Loss: 0.6680\n",
            "Epoch [80/1000], Loss: 0.6559\n",
            "Epoch [90/1000], Loss: 0.6583\n",
            "Epoch [100/1000], Loss: 0.6548\n",
            "Epoch [110/1000], Loss: 0.6465\n",
            "Epoch [120/1000], Loss: 0.6461\n",
            "Epoch [130/1000], Loss: 0.6438\n",
            "Epoch [140/1000], Loss: 0.6334\n",
            "Epoch [150/1000], Loss: 0.6293\n",
            "Epoch [160/1000], Loss: 0.6234\n",
            "Epoch [170/1000], Loss: 0.6114\n",
            "Epoch [180/1000], Loss: 0.6255\n",
            "Epoch [190/1000], Loss: 0.6170\n",
            "Epoch [200/1000], Loss: 0.6104\n",
            "Epoch [210/1000], Loss: 0.6116\n",
            "Epoch [220/1000], Loss: 0.6014\n",
            "Epoch [230/1000], Loss: 0.6028\n",
            "Epoch [240/1000], Loss: 0.6065\n",
            "Epoch [250/1000], Loss: 0.6003\n",
            "Epoch [260/1000], Loss: 0.5941\n",
            "Epoch [270/1000], Loss: 0.5924\n",
            "Epoch [280/1000], Loss: 0.6048\n",
            "Epoch [290/1000], Loss: 0.6009\n",
            "Epoch [300/1000], Loss: 0.5836\n",
            "Epoch [310/1000], Loss: 0.5867\n",
            "Epoch [320/1000], Loss: 0.5653\n",
            "Epoch [330/1000], Loss: 0.5896\n",
            "Epoch [340/1000], Loss: 0.5805\n",
            "Epoch [350/1000], Loss: 0.5671\n",
            "Epoch [360/1000], Loss: 0.5671\n",
            "Epoch [370/1000], Loss: 0.5941\n",
            "Epoch [380/1000], Loss: 0.5884\n",
            "Epoch [390/1000], Loss: 0.5717\n",
            "Epoch [400/1000], Loss: 0.5767\n",
            "Epoch [410/1000], Loss: 0.5808\n",
            "Epoch [420/1000], Loss: 0.5719\n",
            "Epoch [430/1000], Loss: 0.5828\n",
            "Epoch [440/1000], Loss: 0.5712\n",
            "Epoch [450/1000], Loss: 0.5617\n",
            "Epoch [460/1000], Loss: 0.5527\n",
            "Epoch [470/1000], Loss: 0.5678\n",
            "Epoch [480/1000], Loss: 0.5640\n",
            "Epoch [490/1000], Loss: 0.5720\n",
            "Epoch [500/1000], Loss: 0.5513\n",
            "Epoch [510/1000], Loss: 0.5730\n",
            "Epoch [520/1000], Loss: 0.5512\n",
            "Epoch [530/1000], Loss: 0.5581\n",
            "Epoch [540/1000], Loss: 0.5724\n",
            "Epoch [550/1000], Loss: 0.5623\n",
            "Epoch [560/1000], Loss: 0.5572\n",
            "Epoch [570/1000], Loss: 0.5572\n",
            "Epoch [580/1000], Loss: 0.5513\n",
            "Epoch [590/1000], Loss: 0.5572\n",
            "Epoch [600/1000], Loss: 0.5625\n",
            "Epoch [610/1000], Loss: 0.5462\n",
            "Epoch [620/1000], Loss: 0.5342\n",
            "Epoch [630/1000], Loss: 0.5491\n",
            "Epoch [640/1000], Loss: 0.5627\n",
            "Epoch [650/1000], Loss: 0.5639\n",
            "Epoch [660/1000], Loss: 0.5356\n",
            "Epoch [670/1000], Loss: 0.5656\n",
            "Epoch [680/1000], Loss: 0.5356\n",
            "Epoch [690/1000], Loss: 0.5552\n",
            "Epoch [700/1000], Loss: 0.5706\n",
            "Epoch [710/1000], Loss: 0.5420\n",
            "Epoch [720/1000], Loss: 0.5273\n",
            "Epoch [730/1000], Loss: 0.5442\n",
            "Epoch [740/1000], Loss: 0.5622\n",
            "Epoch [750/1000], Loss: 0.5472\n",
            "Epoch [760/1000], Loss: 0.5535\n",
            "Epoch [770/1000], Loss: 0.5474\n",
            "Epoch [780/1000], Loss: 0.5346\n",
            "Epoch [790/1000], Loss: 0.5282\n",
            "Epoch [800/1000], Loss: 0.5465\n",
            "Epoch [810/1000], Loss: 0.5306\n",
            "Epoch [820/1000], Loss: 0.5331\n",
            "Epoch [830/1000], Loss: 0.5348\n",
            "Epoch [840/1000], Loss: 0.5384\n",
            "Epoch [850/1000], Loss: 0.5275\n",
            "Epoch [860/1000], Loss: 0.5362\n",
            "Epoch [870/1000], Loss: 0.5361\n",
            "Epoch [880/1000], Loss: 0.5503\n",
            "Epoch [890/1000], Loss: 0.5479\n",
            "Epoch [900/1000], Loss: 0.5445\n",
            "Epoch [910/1000], Loss: 0.5447\n",
            "Epoch [920/1000], Loss: 0.5187\n",
            "Epoch [930/1000], Loss: 0.5475\n",
            "Epoch [940/1000], Loss: 0.5151\n",
            "Epoch [950/1000], Loss: 0.5289\n",
            "Epoch [960/1000], Loss: 0.5175\n",
            "Epoch [970/1000], Loss: 0.5418\n",
            "Epoch [980/1000], Loss: 0.5252\n",
            "Epoch [990/1000], Loss: 0.5038\n",
            "Epoch [1000/1000], Loss: 0.5227\n",
            "Training time for Freqtist NN: 2.4546 seconds\n",
            "Evaluating model...\n",
            "Test Accuracy: 74.03%\n",
            "Inference time for Freqtist NN: 0.0042 seconds\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Docker-BNN)",
      "language": "python",
      "name": "docker-bnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
